{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fractional Information Gain (FIG) and skill score (FIGSS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a single grid-point. We would like to know the amount of information gained IG about a specific hazard (JRL: message) $\\zeta$ in the set of all pertinent hazards $\\mathbb{Z}$. In this case, whether 60-min precipitation accumulation between 1800 and 1900 UTC exceeded 25 $\\textrm{mm}\\,\\textrm{h}^{-1}$. However, note:\n",
    "\n",
    "* The user values the signal of flash flooding most (i.e., identification of high-precipitation areas), but can tolerate small errors in time $\\epsilon_{t}$, latitude $\\epsilon_{y}$, and longitude $\\epsilon_{x}$.\n",
    "* NWP output is probabilistic, where $N$ ensemble members in the set $\\mathbb{E} = {e_1, e_2, ... ,e_N}$ return a binary digit forecast $e_i \\in {0,1}$. \n",
    "* We assume a 99.5% likelihood in the estimated observed value $\\mathsf{P}_{\\textrm{obs}}$ to reflect a small uncertainty in verification truth.\n",
    "\n",
    "To avoid infinite confidence, we must bound the forecast probability $\\mathsf{P}_{\\textrm{NWP}}(x)$, and we choose the same bounds as the observational accuracy $\\epsilon_{o}$. This is not required for the posterior observation verifying the occurrence of $\\zeta$, which could also be assumed perfect (i.e., $\\mathsf{P}_{\\textrm{obs}} \\in {0,1}$). \n",
    "\n",
    "Hence, we seek to minimise the prior uncertainty, entropy $H$ (or ignorance, IGN) that exists before the NWP output is available (Roulston & Smith 2002):\n",
    "\n",
    "$$ \\textrm{IGN} = -\\log_2 \\mathsf{P}_{\\textrm{NWP}}(\\zeta) $$\n",
    "\n",
    "So, for example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as N\n",
    "\n",
    "# 10-member ensemble output: does QPF exceed 25 mm/h?\n",
    "fcst_arr = N.array([1,0,0,1,0,0,0,1,0,0])\n",
    "# This yields a probability of 30%.\n",
    "P_NWP = N.mean(fcst_arr)\n",
    "\n",
    "# The forecast probability is:\n",
    "print(P_NWP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior observation, with uncertainty: \n",
    "obs_arr = N.array([0.995])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7369655941662063\n"
     ]
    }
   ],
   "source": [
    "# Hence, the ignorance remaining in the forecast is, in bits:\n",
    "\n",
    "H = -N.log2(P_NWP)\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15200309344504995\n"
     ]
    }
   ],
   "source": [
    "# If the ensemble was far more confident, we see the difference:\n",
    "P_NWP = 0.90\n",
    "H = -N.log2(P_NWP)\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now decompose the ignorance remaining in the forecast as follows (Toedter and Ahrens, 2012):\n",
    "\n",
    "$$ \\textrm{IGN} = \\textrm{REL} - \\textrm{RES} + \\textrm{UNC} $$\n",
    "\n",
    "The three terms of the right side can be interpreted as follows:\n",
    "\n",
    "* Reliability (REL) is...\n",
    "* Resolution (RES) is...\n",
    "* Uncertainty (UNC) is the upper bound of IG, and represents the entropy (uncertainty) in the observational field.\n",
    "\n",
    "As a scoring rule, IGN (and its subcomponents) can be summed over all hazards (or variables) and times. The larger the sample size of (preferably independent) times, the more confidence one can place in the ultimate inter-model comparisons. We compute the components as follows:\n",
    "\n",
    "$$ \\textrm{REL} = x $$\n",
    "\n",
    "where...\n",
    "\n",
    "$$ \\textrm{RES} = x $$\n",
    "\n",
    "where..., and\n",
    "\n",
    "$$ \\textrm{UNC} = x $$\n",
    "\n",
    "For one time, this yields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "def compute_REL():\n",
    "    pass\n",
    "\n",
    "def compute_RES():\n",
    "    pass\n",
    "\n",
    "def compute_UNC():\n",
    "    pass\n",
    "\n",
    "REL = compute_REL()\n",
    "print(REL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "RES = compute_RES()\n",
    "print(RES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "UNC = compute_UNC()\n",
    "print(UNC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The total ignorance remaining is:\n",
    "print(REL-RES+UNC, \"bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us increase to five independent times in the sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst_arr = N.array([[1,0,0,1,0,0,0,1,0,0], # 30%; not observed\n",
    "                   [1,0,1,1,1,1,0,1,1,0], # 70%; observed\n",
    "                   [0,0,0,1,0,0,0,1,0,0], # 20%; observed\n",
    "                   [0,0,0,0,0,0,0,0,0,0], # 0%; not observed\n",
    "                   [1,1,1,1,1,1,1,1,1,1]])# 100%; observed\n",
    "\n",
    "# Compute probabilities\n",
    "P_NWPs = N.mean(fcst_arr,axis=0)\n",
    "\n",
    "# Avoid divergence to infinity\n",
    "P_NWPs[P_NWPs > 0.995] = 0.995\n",
    "P_NWPs[P_NWPs < 0.005] = 0.005\n",
    "\n",
    "print(P_NWPs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And here are the observed values for each time:\n",
    "obs_arr = N.array([0.005,0.995,0.995,0.005,0.995])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returning to the three subcomponents of IGN, we assess each by eye:\n",
    "\n",
    "* The resolution, or \"inherent goodness\" of the forecast, looks \"good\": there is often a good correlation between high probabilities and an observed event (i.e., high mutual information between the forecast and observed probabilities).\n",
    "* The reliability is less impressive: the third time (20% but observed) is an example of an underconfidence ensemble forecast of flash flooding. However, we remember for later that this 20% may still be useful: if the event in question $\\zeta$ was inherently less predictable for that time (e.g., weakly forced convection and precipitation on a calm summer day with moderate instability), then a 1-in-5 detection of the hazard is better than nothing.\n",
    "* Indeed, the uncertainty is (large/small?)...\n",
    "\n",
    "Now, let us compute the three components for the larger sample set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REL = compute_REL()\n",
    "print(REL)\n",
    "\n",
    "RES = compute_RES()\n",
    "print(RES)\n",
    "\n",
    "UNC = compute_UNC()\n",
    "print(UNC)\n",
    "\n",
    "# The total ignorance remaining is:\n",
    "print(REL-RES+UNC, \"bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now consider a 3-by-3 grid for one time. This represents nine points in latitude--longitude space, where the user wants to gain information about hazard $\\zeta$: flash flooding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do 3-by-3\n",
    "\n",
    "# Plot the probability field, raw and then smoothed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now add in a temporal tolerance. We consider three consective times: one timestep either side of the given time being verified. This is a cube in time--latitude--longitude space (we have already reduced the ensemble-member dimension via the mean function - this step will be delayed for computational efficiency later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do 3-by-3-by-3\n",
    "\n",
    "# Plot raw, then mean over time/space (show them independently), and when it's done together (as a FFT)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "FIG is the amount of information gained about the probability of hazard occurrence $\\mathsf{P}(\\zeta)$, within a window of time and space. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
